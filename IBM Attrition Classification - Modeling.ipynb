{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X_resampled_down.pickle\n",
      "Loaded X_resampled_up.pickle\n",
      "Loaded X_test.pickle\n",
      "Loaded X_train.pickle\n",
      "Loaded y_resampled_down.pickle\n",
      "Loaded y_resampled_up.pickle\n",
      "Loaded y_test.pickle\n",
      "Loaded y_train.pickle\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "#Import Data \n",
    "path = './data/split_data/'\n",
    "directory = os.listdir(path)\n",
    "for file in directory:\n",
    "    if file.endswith(\".pickle\"):\n",
    "        with open(path+file, 'rb') as f:\n",
    "            placeholder = file.split('.')[0] #Placeholder to modify file name to variable name\n",
    "            vars()[placeholder] = pickle.load(f)\n",
    "            print('Loaded', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working Copy Figure out how to fit into workbook\n",
    "# #Import Data From Director and Assign File Name to Variable Name\n",
    "# def import_data(directory_path, file_extension =\".pickle\"):\n",
    "#     data_list = []\n",
    "#     directory = os.listdir(path)\n",
    "#     for file in directory:\n",
    "#         if file.endswith(file_extension):\n",
    "#             with open(path+file, 'rb') as f:\n",
    "#                 placeholder = file.split('.')[0] #Placeholder to modify file name to variable name\n",
    "#                 vars()[placeholder] = pickle.load(f)\n",
    "#                 data_list.append(vars()[placeholder])\n",
    "#                 print('Loaded', file)\n",
    "#     return data_list\n",
    "\n",
    "# path = './data/split_data/'\n",
    "# split_data = import_data(path, file_extension =\".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = [X_train,y_train,X_resampled_up, y_resampled_up, X_resampled_down,y_resampled_down]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required pacakages\n",
    "#Imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "#Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "# make prettier plots\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "\n",
    "#Numpy\n",
    "import numpy as np\n",
    "\n",
    "#Pandas\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "\n",
    "#Seaborn\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#Sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.utils import resample\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             f1_score,\n",
    "                             roc_auc_score,\n",
    "                             roc_curve,\n",
    "                             confusion_matrix)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import (RandomForestClassifier,\n",
    "                              RandomForestClassifier,\n",
    "                              RandomForestRegressor,\n",
    "                              GradientBoostingClassifier, \n",
    "                              ExtraTreesClassifier, #For each feature split rule is random, not optimal\n",
    "                              VotingClassifier, \n",
    "                              AdaBoostClassifier, \n",
    "                              BaggingRegressor)\n",
    "\n",
    "from sklearn.model_selection import (cross_val_score,\n",
    "                                     GridSearchCV,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     learning_curve,\n",
    "                                     validation_curve,\n",
    "                                     train_test_split,\n",
    "                                     cross_validate)\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "#XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methodology that I’ll follow when building the classifiers goes as follows:\n",
    "\n",
    "Build a pipeline that handles all the steps when fitting the classifier using scikit-learn’s `make_pipeline` which will have two steps:\n",
    "\n",
    "I. Standardizing the data to speed up convergence and make all features on the same scale.\n",
    "\n",
    "II. The classifier (estimator) we want to use to fit the model.\n",
    "\n",
    "2. Use GridSearchCV to tune hyperparameters using 10-folds cross validation. We can use RandomizedSearchCV which is faster and may outperform GridSearchCV especially if we have more than two hyperparameters and the range for each one is very big; however, GridSearchCV will work just fine since we have only two hyperparameters and descent range.\n",
    "\n",
    "3. Fit the model using training data.\n",
    "\n",
    "4. Plot both confusion matrix and ROC curve for the best estimator using test data.\n",
    "\n",
    "Repeat the above steps for `Random Forest`, `Gradient Boosting Trees`, `K-Nearest Neighbors`, `Logistic Regression` and `Support Vector Machine`. Next, pick the classifier that has the highest cross validation f1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - Random Forest\n",
    "First, fit a random forest classifier using `unsampled`, `upsampled`, and `downsampled` data. Second, evaluate each method using cross validation CV f1-score and pick the one with the highest CV F1 score. The best method will be used to fit the rest of the classifiers\n",
    "\n",
    "Hyperparameters to tune:\n",
    "\n",
    "`max_features`: How many features to consider randomly on each split to help avoid having few strong features to be picked on each split and let other features have the chance to contribute. The predictions will be less correlated and the variance of each tree will decrease.\n",
    "\n",
    "`min_samples_leaf`: How many examples to have for each split to be a final leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDecision Trees are prone to high variance. Comes at cost of overfitting the training set.\\n\\nHuristic: Greedy Search\\nInformatino Entropy\\nSplit wiht entrop minimization rule\\nBoostrap Aggregating - Sample with Replacement\\n\\nEnsemble Methods Random Forests\\nIntroduce randomness when building each tree.\\nFor a split, do not take the “best” feature split.\\nFirst, randomly choose sqrt(n_feat) features.\\nOnly choose the best split among these.\\n\\nBagging - Reduces Bagging\\n-Average Voting\\n-Max Voting\\nlooking at bootstrapped samples and running several different models on the data allows the classifier to \\nfind the best aggregate model that will generalize best to our data.\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Decision Trees are prone to high variance. Comes at cost of overfitting the training set.\n",
    "\n",
    "Huristic: Greedy Search\n",
    "Informatino Entropy\n",
    "Split wiht entrop minimization rule\n",
    "Boostrap Aggregating - Sample with Replacement\n",
    "\n",
    "Ensemble Methods Random Forests\n",
    "Introduce randomness when building each tree.\n",
    "For a split, do not take the “best” feature split.\n",
    "First, randomly choose sqrt(n_feat) features.\n",
    "Only choose the best split among these.\n",
    "\n",
    "Bagging - Reduces Bagging\n",
    "-Average Voting\n",
    "-Max Voting\n",
    "looking at bootstrapped samples and running several different models on the data allows the classifier to \n",
    "find the best aggregate model that will generalize best to our data.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_metric(y_true, y_pred):\n",
    "    return np.mean((np.mean(y_pred) - y_true)**2)\n",
    "\n",
    "def variance_metric(y_true, y_pred):\n",
    "    return np.var(y_true - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "\n",
    "placeholder_variance=[]\n",
    "placeholder_bias=[]\n",
    "\n",
    "variance = []\n",
    "bias = []\n",
    "test_range = np.arange(1, 100, 1) #Number of Trees\n",
    "\n",
    "while i < len(list_data)-1:\n",
    "    X=list_data[i]\n",
    "    y=list_data[i+1]\n",
    "    for r in test_range:\n",
    "        cv_out = cross_validate(\n",
    "            estimator=BaggingRegressor(\n",
    "                DecisionTreeRegressor(random_state=123), n_estimators=r),\n",
    "            X=X,\n",
    "            y=y,\n",
    "            cv=3,\n",
    "            return_train_score=True,\n",
    "            scoring={\n",
    "                \"variance\": make_scorer(variance_metric),\n",
    "                \"bias\": make_scorer(bias_metric)\n",
    "            },\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        placeholder_variance.append(np.mean(cv_out['test_variance']))\n",
    "        placeholder_bias.append(np.mean(cv_out['test_bias']))\n",
    "        \n",
    "    variance.append(placeholder_variance)\n",
    "    bias.append(placeholder_bias)\n",
    "    placeholder_variance=[] #clear placeholder\n",
    "    placeholder_bias=[] #clear placeholder\n",
    "    i+=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['Orignial Data','Smoted','Down-Sampled Training Data','Up-Sampled Training Data', 'Down-Resampled Training Data', 'Up-Resampled Training Data']\n",
    "for i in range(len(variance)):\n",
    "    plt.figure(figsize=(8,8),dpi=600)\n",
    "    plt.plot(test_range, variance[i], label='variance')\n",
    "    #plt.ylim((0, 0.65))\n",
    "    plt.xlabel('Number of Estimators')\n",
    "    plt.ylabel('Variance Score');\n",
    "    plt.title('Variance Score vs No. of Estimators - ' +titles[i]);\n",
    "\n",
    "    plt.figure(dpi=150)\n",
    "    plt.plot(test_range, bias[i], label='bias', c='r')\n",
    "    #plt.ylim((0, 0.25))\n",
    "    plt.xlabel('Number of Estimators')\n",
    "    plt.ylabel('Bias Score');\n",
    "    plt.title('Bias Score vs No. of Estimators - ' +titles[i]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Balance of Classes\n",
    "- Look at proportion of classes to see if we're dealing with balanced or imbalanced data, since each one has its own set of tools to be used when fitting classifiers\n",
    "\n",
    "# Get number of Yes/No Attritions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./y_test_smote_set.pickle', 'rb') as file:\n",
    "        y_test_smote_set = pickle.load(file)\n",
    "        \n",
    "with open('./x_test_smote_set.pickle', 'rb') as file:\n",
    "        x_test_smote_set = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build random forest classifier\n",
    "def random_forest_classifier(X_train, y_train):\n",
    "    methods_data = {\"Original\": (X_train, y_train)}\n",
    "                    #\"SMOTE\": (X_smoted, y_smoted),\n",
    "                    #\"Up-Sampled\": (X_train_up, y_train_up),\n",
    "                    #\"Down-Sampled\": (X_train_down, y_train_down),\n",
    "                    #\"Up-Resampled\": (X_resampled_up, y_resampled_up),\n",
    "                    #\"Down-Resampled\": (X_resampled_down,y_resampled_down)}\n",
    "\n",
    "    for method in methods_data.keys():\n",
    "        pip_rf = make_pipeline(StandardScaler(),\n",
    "                               RandomForestClassifier(n_estimators=500, #Typically Sufficient\n",
    "                                                      class_weight=\"balanced\",\n",
    "                                                      random_state=123))\n",
    "\n",
    "        hyperparam_grid = {\n",
    "            \"randomforestclassifier__n_estimators\": [10, 50, 100, 200, 500],\n",
    "            \"randomforestclassifier__max_features\": [\"sqrt\", \"log2\", 0.4, 0.5], #sqrt/log2 => max_features = sqrt/log2(n_features), \n",
    "            \"randomforestclassifier__min_samples_leaf\": [1, 3, 5, 10, 20, 30],\n",
    "            \"randomforestclassifier__criterion\": [\"gini\", \"entropy\"]}\n",
    "        \n",
    "        gs_rf = GridSearchCV(pip_rf,\n",
    "                             hyperparam_grid,\n",
    "                             scoring=\"precision\", #The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "                             #where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of \n",
    "                             #precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "                             cv=10, # Number of K-fold cross validation\n",
    "                             n_jobs=-1) # Setting n_jobs to -1, specifys that all CPUs are used\n",
    "        \n",
    "        # training the model on oversampled 4 folds of training set\n",
    "        gs_rf.fit(methods_data[method][0], methods_data[method][1])\n",
    "\n",
    "        print(f\"\\033[1m\\033[0mThe best hyperparameters for {method} data:\")\n",
    "        for hyperparam in gs_rf.best_params_.keys():\n",
    "            print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_rf.best_params_[hyperparam])\n",
    "\n",
    "        print(f\"\\033[1m\\033[94mBest 10-folds CV precision-score: {gs_rf.best_score_ * 100:.2f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier(X_train, y_train)\n",
    "# i=0\n",
    "# import math\n",
    "# while i < len(list_data)-1:\n",
    "#     print(titles[math.floor(i/2)])\n",
    "#     X=list_data[i]\n",
    "#     y=list_data[i+1]\n",
    "#     random_forest_classifier(X, y)\n",
    "#     i+=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Hyperparameters as discovered by RandomForestClassifiera with Best 10-fold CV F1-Score of 98.80%:\n",
    "\n",
    "- criterion :  gini\n",
    "- max_features :  sqrt\n",
    "- min_samples_leaf :  1\n",
    "- n_estimators :  200\n",
    "- Best 10-folds CV f1-score: 98.80%.\n",
    "\n",
    "The upsampled data will be used to fit the rest of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest_Classifier(X_train, y_train,n_estimators,criterion,max_features,min_samples_leaf):\n",
    "    # Reassign original training data to upsampled data\n",
    "    X_train_re, y_train_re = np.copy(X_train), np.copy(y_train)\n",
    "\n",
    "    # Refit RF classifier using best params\n",
    "    clf_rf = make_pipeline(StandardScaler(),\n",
    "                           RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                                  criterion=criterion,\n",
    "                                                  max_features=max_features,\n",
    "                                                  min_samples_leaf=min_samples_leaf,\n",
    "                                                  class_weight=\"balanced\",\n",
    "                                                  n_jobs=-1,\n",
    "                                                  random_state=123))\n",
    "    clf_rf.fit(X_train_re, y_train_re)\n",
    "        \n",
    "    return clf_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Balance of Classes\n",
    "- Look at proportion of classes to see if we're dealing with balanced or imbalanced data, since each one has its own set of tools to be used when fitting classifiers\n",
    "\n",
    "# Get number of Yes/No Attritions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import validation and test data\n",
    "# path = './data/split_data/val_test/'\n",
    "# directory = os.listdir(path)\n",
    "# for file in directory:\n",
    "#     if file.endswith(\".pickle\"):\n",
    "#         with open(path+file, 'rb') as f:\n",
    "#             placeholder = file.split('.')[0] #Placeholder to modify file name to variable name\n",
    "#             vars()[placeholder] = pickle.load(f)\n",
    "#             print('Loaded', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_data_list = [X_val,y_val]\n",
    "#test_data_list = [X_test,y_test]\n",
    "test_data_list = [x_test_smote_set,y_test_smote_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 300\n",
    "criterion = 'gini'\n",
    "max_features = 'log2'\n",
    "min_samples_leaf = 1\n",
    "\n",
    "#Will use up-sampled data per random forest results\n",
    "X_train = x_train_smoted\n",
    "y_train = y_train_smoted\n",
    "\n",
    "#Run RandomForest Classifier per Data\n",
    "clf_RandomForest = Random_Forest_Classifier(X_train, y_train, n_estimators,criterion,max_features,min_samples_leaf)\n",
    "\n",
    "#Save Model\n",
    "#filename = './data/fit_models/random_forest/Up_Sampled_Data_RandomForest.sav'\n",
    "#pickle.dump(clf_RandomForest, open(filename, 'wb'))\n",
    "\n",
    "#Predict Y using Fitted Models \n",
    "y_pred = clf_RandomForest.predict(test_data_list[0])\n",
    "#filename = './data/fit_models/random_forest/ypred_Up_Sampled_Data_RandomForest.sav'\n",
    "#pickle.dump(y_pred, open(filename, 'wb'))\n",
    "\n",
    "# new_list_data_per_tunining = [X_smoted,y_smoted,X_adasyn,y_adasyn,X_train_up,y_train_up,X_resampled_up,y_resampled_up]\n",
    "# titles = ['SMOTE','ADASYN','TrainUp','ResampledUp']\n",
    "# i=0\n",
    "# while i < len(new_list_data_per_tunining)-1:\n",
    "#     X=new_list_data_per_tunining[i]\n",
    "#     y=new_list_data_per_tunining[i+1]\n",
    "#     clf_RandomForest = Random_Forest_Classifier(X, y, n_estimators,criterion,max_features,min_samples_leaf)\n",
    "#     #Save Model\n",
    "#     filename = './data/fit_models/'+titles[math.floor(i/2)]+'.sav'\n",
    "#     pickle.dump(clf_RandomForest, open(filename, 'wb'))\n",
    "    \n",
    "#     #Predict Y using Fitted Models \n",
    "#     y_pred = clf_RandomForest.predict(val_data_list[0])\n",
    "#     filename = './data/fit_models/y_pred/ypred_'+titles[math.floor(i/2)]+'.sav'\n",
    "#     pickle.dump(y_pred, open(filename, 'wb'))\n",
    "#     i+=2\n",
    "# print('Fitting Models Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import y_pred\n",
    "\n",
    "\n",
    "#path = './data/fit_models/y_pred/'\n",
    "# directory = os.listdir(path)\n",
    "# y_pred_list= []\n",
    "# for file in directory:\n",
    "#     if file.endswith(\".sav\"):\n",
    "#         with open(path+file, 'rb') as f:\n",
    "#             placeholder = file.split('.')[0] #Placeholder to modify file name to variable name\n",
    "#             vars()[placeholder] = pickle.load(f)\n",
    "#             y_pred_list.append(vars()[placeholder])\n",
    "#             print('Loaded', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Balance of Classes\n",
    "- Look at proportion of classes to see if we're dealing with balanced or imbalanced data, since each one has its own set of tools to be used when fitting classifiers\n",
    "\n",
    "# Get number of Yes/No Attritions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s refit the Random Forest with Upsampled data using best hyperparameters tuned above and plot confusion matrix and ROC curve using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    plt.figure(dpi=600)\n",
    "    \n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    #plt.setp(ax.get_xticklabels(), rotation=0, ha=\"right\",rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                    fontsize=20)\n",
    "    fig.tight_layout()\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "              ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(16)\n",
    "    ax.grid(None)\n",
    "    ax.yaxis.set_label_coords(-0.15,0.5)\n",
    "    ax.xaxis.set_label_coords(0.5,-0.15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Normalized Confusion Matrix\n",
    "cm_gb = plot_confusion_matrix(y_test_smote_set, y_pred, title='IBM Attrition Data Random Forest Results Confusion Matrix');\n",
    "print('Random Forest Classifcation Report: \\n ', classification_report(y_test_smote_set,y_pred));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot non-normalized confusion matrix\n",
    "# titles = ['SMOTE','ADASYN','TrainUp','ResampledUp']\n",
    "# for idx, y_pred in enumerate(y_pred_list):\n",
    "#     cm = plot_confusion_matrix(y_val, y_pred, title='Confusion Matrix '+titles[idx]+' without normalization');\n",
    "#     print('Classifcation Report for: \\n'+titles[idx], classification_report(y_val,y_pred));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = './data/fit_models/'\n",
    "# directory = os.listdir(path)\n",
    "# y_pred_list= []\n",
    "# for file in directory:\n",
    "#     if file.endswith(\".sav\"):\n",
    "#         with open(path+file, 'rb') as f:\n",
    "#             placeholder = file.split('.')[0] #Placeholder to modify file name to variable name\n",
    "#             vars()[placeholder] = pickle.load(f)\n",
    "#             y_pred_list.append(vars()[placeholder])\n",
    "#             print('Loaded', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitted_models = [SMOTE,ADASYN,TrainUp,ResampledUp]\n",
    "fitted_models = [clf_RandomForest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titles = ['SMOTE','ADASYN','TrainUp','ResampledUp']\n",
    "# for idx, model in enumerate(fitted_models):\n",
    "#     fpr, tpr, thresholds = roc_curve(y_val, model.predict_proba(X_val)[:,1])\n",
    "#     plt.plot(fpr, tpr,lw=2)\n",
    "#     plt.plot([0,1],[0,1],c='violet',ls='--')\n",
    "#     plt.xlim([-0.05,1.05])\n",
    "#     plt.ylim([-0.05,1.05])\n",
    "\n",
    "#     plt.xlabel('False positive rate')\n",
    "#     plt.ylabel('True positive rate')\n",
    "#     plt.title('Random Forest ROC curve for IBM Employee Retention');\n",
    "#     print(\"Random Forest \"+titles[idx]+\" ROC AUC score = \", roc_auc_score(y_val, model.predict_proba(X_val)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate ROC Curve for Random Forest\n",
    "fpr, tpr, thresholds = roc_curve(y_test_smote_set,clf_RandomForest.predict_proba(x_test_smote_set)[:,1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.figure(dpi=600)\n",
    "\n",
    "ax.plot(fpr, tpr,lw=2)\n",
    "ax.plot([0,1],[0,1],c='violet',ls='--')\n",
    "ax.set_xlim([-0.05,1.05])\n",
    "ax.set_ylim([-0.05,1.05])\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=16)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=16)\n",
    "ax.set_title('Random Forest ROC curve for IBM Employee Retention',fontsize=16)\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "              ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(16)\n",
    "\n",
    "ax.yaxis.set_label_coords(-0.1,0.5)\n",
    "ax.xaxis.set_label_coords(0.5,-0.1)\n",
    "print(f\"\\033[1m\\033[94mRandom Forest Up-Sampled Data ROC AUC score = \", roc_auc_score(y_test_smote_set, clf_RandomForest.predict_proba(x_test_smote_set)[:,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *NOTE* PAST THIS POINT NOTHING IS AUTOMATED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Trees\n",
    "\n",
    "Gradient Boosting trees are the same as Random Forest except for:\n",
    "\n",
    "It starts with small tree and start learning from grown trees by taking into account the residual of grown trees.\n",
    "More trees can lead to overfitting; opposite to Random Forest.\n",
    "The two other hyperparameters than max_features and n_estimators that we're going to tune are:\n",
    "\n",
    "learning_rate: rate the tree learns, the slower the better.\n",
    "max_depth: number of split each time a tree is growing which limits the number of nodes in each tree.\n",
    "Let's fit GB classifier and plot confusion matrix and ROC curve using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Gradient Boosting classifier\n",
    "pip_gb = make_pipeline(StandardScaler(),\n",
    "                       GradientBoostingClassifier(loss=\"deviance\",\n",
    "                                                  random_state=123))\n",
    "\n",
    "hyperparam_grid = {\"gradientboostingclassifier__max_features\": [\"log2\", 0.5],\n",
    "                   \"gradientboostingclassifier__n_estimators\": [100, 300, 500],\n",
    "                   \"gradientboostingclassifier__learning_rate\": [0.001, 0.01, 0.1],\n",
    "                   \"gradientboostingclassifier__max_depth\": [1, 2, 3]}\n",
    "\n",
    "gs_gb = GridSearchCV(pip_gb,\n",
    "                      param_grid=hyperparam_grid,\n",
    "                      scoring= \"precision\",\n",
    "                      cv=10,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "gs_gb = gs_gb.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters:\")\n",
    "print(\"-\" * 25)\n",
    "for hyperparam in gs_gb.best_params_.keys():\n",
    "    print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_gb.best_params_[hyperparam])\n",
    "\n",
    "print(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_gb.best_score_) * 100))\n",
    "\n",
    "#Save Model\n",
    "filename = './data/fit_models/gradient_boosting/gradient_boost_trees_model.sav'\n",
    "pickle.dump(gs_gb, open(filename, 'wb'))\n",
    "\n",
    "#Predict Y using Model\n",
    "y_pred = gs_gb.predict(test_data_list[0])\n",
    "\n",
    "filename = './data/fit_models/gradient_boosting/gradient_boost_trees_ypred.sav'\n",
    "pickle.dump(y_pred, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Normalized Confusion Matrix\n",
    "cm_gb = plot_confusion_matrix(y_test, y_pred, title='IBM Attrition Data Gradient Boosting Forest Results Confusion Matrix');\n",
    "print('Gradient Boosting Tree Classifcation Report: \\n ', classification_report(y_test,y_pred));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate ROC Curve for Gradient Boosting Tree\n",
    "fpr, tpr, thresholds = roc_curve(y_test,gs_gb.predict_proba(X_test)[:,1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.figure(dpi=600)\n",
    "\n",
    "ax.plot(fpr, tpr,lw=2)\n",
    "ax.plot([0,1],[0,1],c='violet',ls='--')\n",
    "ax.set_xlim([-0.05,1.05])\n",
    "ax.set_ylim([-0.05,1.05])\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=16)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=16)\n",
    "ax.set_title('Gradient Boosting Trees ROC curve for IBM Employee Retention',fontsize=16)\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "              ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(16)\n",
    "\n",
    "ax.yaxis.set_label_coords(-0.1,0.5)\n",
    "ax.xaxis.set_label_coords(0.5,-0.1)\n",
    "print(f\"\\033[1m\\033[94mGradient Boosting Up-Sampled Data ROC AUC score = \", roc_auc_score(y_test, gs_gb.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "KNN is called a lazy learning algorithm because it doesn't learn or fit any parameter. It takes n_neighbors points from the training data closest to the point we're interested to predict it's class and take the mode (majority vote) of the classes for the neighboring point as its class. The two hyperparameters we're going to tune are:\n",
    "\n",
    "- n_neighbors: number of neighbors to use in prediction.\n",
    "- weights: how much weight to assign neighbors based on:\n",
    "  \n",
    "  \"uniform\": all neighboring points have the same weight.\n",
    "  \n",
    "  \"distance\": use the inverse of euclidean distance of each neighboring point used in prediction.\n",
    "\n",
    "Let's fit KNN classifier and plot confusion matrix and ROC curve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build KNN classifier\n",
    "pip_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "hyperparam_range = range(1, 20)\n",
    "\n",
    "gs_knn = GridSearchCV(pip_knn,\n",
    "                      param_grid={\"kneighborsclassifier__n_neighbors\": hyperparam_range,\n",
    "                                  \"kneighborsclassifier__weights\": [\"uniform\", \"distance\"]},\n",
    "                      scoring=\"f1\",\n",
    "                      cv=10,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "gs_knn = gs_knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters:\")\n",
    "print(\"-\" * 25)\n",
    "for hyperparam in gs_knn.best_params_.keys():\n",
    "    print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_knn.best_params_[hyperparam])\n",
    "\n",
    "print(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_knn.best_score_) * 100))\n",
    "\n",
    "#Save Model\n",
    "filename = './data/fit_models/KNN/KNN_model.sav'\n",
    "pickle.dump(gs_knn, open(filename, 'wb'))\n",
    "\n",
    "#Predict Y using Model\n",
    "y_pred = gs_knn.predict(test_data_list[0])\n",
    "\n",
    "filename = './data/fit_models/KNN/KNN_ypred.sav'\n",
    "pickle.dump(y_pred, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Normalized Confusion Matrix\n",
    "cm_gb = plot_confusion_matrix(y_test, y_pred, title='IBM Attrition Data KNN Results Confusion Matrix');\n",
    "print('KNN Classifcation Report: \\n ', classification_report(y_test,y_pred));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate ROC Curve for KNN\n",
    "fpr, tpr, thresholds = roc_curve(y_test,gs_knn.predict_proba(X_test)[:,1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.figure(dpi=600)\n",
    "\n",
    "ax.plot(fpr, tpr,lw=2)\n",
    "ax.plot([0,1],[0,1],c='violet',ls='--')\n",
    "ax.set_xlim([-0.05,1.05])\n",
    "ax.set_ylim([-0.05,1.05])\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=16)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=16)\n",
    "ax.set_title('KNN ROC curve for IBM Employee Retention',fontsize=16)\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "              ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(16)\n",
    "\n",
    "ax.yaxis.set_label_coords(-0.1,0.5)\n",
    "ax.xaxis.set_label_coords(0.5,-0.1)\n",
    "print(f\"\\033[1m\\033[94mKNN Up-Sampled Data ROC AUC score = \", roc_auc_score(y_test, gs_knn.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "For logistic regression, we'll tune three hyperparameters:\n",
    "- penalty: type of regularization, L2 or L1 regularization.\n",
    "- C: the opposite of regularization of parameter λ. The higher C the less regularization. We'll use values that cover the full  range between unregularized to fully regularized where model is the mode of the examples' label.\n",
    "- fit_intercept: whether to include intercept or not.\n",
    "\n",
    "We won't use any non-linearities such as polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build logistic model classifier\n",
    "pip_logmod = make_pipeline(StandardScaler(),\n",
    "                           LogisticRegression(class_weight=\"balanced\"))\n",
    "\n",
    "hyperparam_range = np.arange(0.5, 20.1, 0.5)\n",
    "\n",
    "hyperparam_grid = {\"logisticregression__penalty\": [\"l1\", \"l2\"],\n",
    "                   \"logisticregression__C\":  hyperparam_range,\n",
    "                   \"logisticregression__fit_intercept\": [True, False]\n",
    "                  }\n",
    "\n",
    "gs_logmodel = GridSearchCV(pip_logmod,\n",
    "                           hyperparam_grid,\n",
    "                           scoring=\"accuracy\",\n",
    "                           cv=2,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "gs_logmodel = gs_logmodel.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\033[1m\" + \"\\033[0m\" + \"The best hyperparameters:\")\n",
    "print(\"-\" * 25)\n",
    "for hyperparam in gs_logmodel.best_params_.keys():\n",
    "    print(hyperparam[hyperparam.find(\"__\") + 2:], \": \", gs_logmodel.best_params_[hyperparam])\n",
    "\n",
    "print(\"\\033[1m\" + \"\\033[94m\" + \"Best 10-folds CV f1-score: {:.2f}%.\".format((gs_logmodel.best_score_) * 100))\n",
    "\n",
    "#Save Model\n",
    "filename = './data/fit_models/logistic_regression/logistic_regression_model.sav'\n",
    "pickle.dump(gs_logmodel, open(filename, 'wb'))\n",
    "\n",
    "#Predict Y using Model\n",
    "y_pred = gs_logmodel.predict(test_data_list[0])\n",
    "\n",
    "filename = './data/fit_models/logistic_regression/logistic_regression_ypred.sav'\n",
    "pickle.dump(y_pred, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Confusion Matrix\n",
    "#Print Normalized Confusion Matrix\n",
    "cm_gb = plot_confusion_matrix(y_test, y_pred, title='IBM Attrition Data Logistic Regression Results Confusion Matrix');\n",
    "print('Logistic Regression Classifcation Report: \\n ', classification_report(y_test,y_pred));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate ROC Curve for Logistic Regression\n",
    "fpr, tpr, thresholds = roc_curve(y_test,gs_logmodel.predict_proba(X_test)[:,1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.figure(dpi=600)\n",
    "\n",
    "ax.plot(fpr, tpr,lw=2)\n",
    "ax.plot([0,1],[0,1],c='violet',ls='--')\n",
    "ax.set_xlim([-0.05,1.05])\n",
    "ax.set_ylim([-0.05,1.05])\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=16)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=16)\n",
    "ax.set_title('Logistic Regression ROC curve for IBM Employee Retention',fontsize=16)\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "              ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(16)\n",
    "\n",
    "ax.yaxis.set_label_coords(-0.1,0.5)\n",
    "ax.xaxis.set_label_coords(0.5,-0.1)\n",
    "print(f\"\\033[1m\\033[94mLogistic Regression Up-Sampled Data ROC AUC score = \", roc_auc_score(y_test, gs_logmodel.predict_proba(X_test)[:,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    "SVM is comutationally very expensive to tune it's hyperparameters for two reasons:\n",
    "- With big datasets, it becomes very slow.\n",
    "- It has good number of hyperparameters to tune that takes very long time to tune on a CPU.\n",
    "\n",
    "Therefore, we'll use recommended hyperparameters' values from the paper we mentioned before that showed to yield the best performane on Penn Machine Learning Benchmark 165 datasets. The hyperparameters that we usually look to tune are:\n",
    "\n",
    "C, gamma, kernel, degree and coef0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svc = make_pipeline(StandardScaler(),\n",
    "                        SVC(C=0.01,\n",
    "                            gamma=0.1,\n",
    "                            kernel=\"poly\",\n",
    "                            degree=5,\n",
    "                            coef0=10,\n",
    "                            probability=True))\n",
    "\n",
    "clf_svc = clf_svc.fit(X_train, y_train)\n",
    "\n",
    "svc_cv_scores = cross_val_score(clf_svc,\n",
    "                                X=X_smoted,\n",
    "                                y=y_smoted,\n",
    "                                scoring=\"f1\",\n",
    "                                cv=10,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "# Print CV\n",
    "print(\"\\033[1m\" + \"\\033[94m\" + \"The 10-folds CV f1-score is: {:.2f}%\".format(\n",
    "       np.mean(svc_cv_scores) * 100))\n",
    "\n",
    "#Save Model\n",
    "filename = './data/fit_models/SVC/SVC_model.sav'\n",
    "pickle.dump(clf_svc, open(filename, 'wb'))\n",
    "\n",
    "#Predict Y using Model\n",
    "y_pred = clf_svc.predict(test_data_list[0])\n",
    "\n",
    "filename = './data/fit_models/SVC/SVC_ypred.sav'\n",
    "pickle.dump(y_pred, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Confusion Matrix\n",
    "#Print Normalized Confusion Matrix\n",
    "cm_gb = plot_confusion_matrix(y_test, y_pred, title='IBM Attrition Data SVM Results Confusion Matrix');\n",
    "print('SVM Classifcation Report: \\n ', classification_report(y_test,y_pred));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate ROC Curve for SVC\n",
    "fpr, tpr, thresholds = roc_curve(y_test,clf_svc.predict_proba(X_test)[:,1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.figure(dpi=600)\n",
    "\n",
    "ax.plot(fpr, tpr,lw=2)\n",
    "ax.plot([0,1],[0,1],c='violet',ls='--')\n",
    "ax.set_xlim([-0.05,1.05])\n",
    "ax.set_ylim([-0.05,1.05])\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=16)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=16)\n",
    "ax.set_title('SVC ROC curve for IBM Employee Retention',fontsize=16)\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "              ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(16)\n",
    "\n",
    "ax.yaxis.set_label_coords(-0.1,0.5)\n",
    "ax.xaxis.set_label_coords(0.5,-0.1)\n",
    "print(f\"\\033[1m\\033[94mSVC Up-Sampled Data ROC AUC score = \", roc_auc_score(y_test, clf_svc.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['RF','LR','SVC','GBT','KNN']\n",
    "# fitted_models = [clf_rf, gs_logmodel, clf_svc, gs_gb, gs_knn]\n",
    "estimators = {\"RF\": clf_RandomForest,\n",
    "              \"LR\": gs_logmodel,\n",
    "              \"SVC\": clf_svc,\n",
    "              \"GBT\": gs_gb,\n",
    "              \"KNN\": gs_knn}\n",
    "\n",
    "print(\"The accuracy rate and f1-score on test data are:\")\n",
    "for estimator in estimators.keys():\n",
    "    print(\"{}: {:.2f}%, {:.2f}%.\".format(estimator,\n",
    "            accuracy_score(y_test, estimators[estimator].predict(X_test)) * 100,\n",
    "            f1_score(y_test, estimators[estimator].predict(X_test)) * 100))\n",
    "        \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, estimators[estimator].predict_proba(X_test)[:,1])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.figure(dpi=600)\n",
    "    \n",
    "    ax.plot(fpr, tpr,lw=2)\n",
    "    ax.plot([0,1],[0,1],c='violet',ls='--')\n",
    "    ax.set_xlim([-0.05,1.05])\n",
    "    ax.set_ylim([-0.05,1.05])\n",
    "    \n",
    "    ax.set_xlabel('False Positive Rate', fontsize=16)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=16)\n",
    "    ax.set_title('Fitted Models ROC curve for IBM Employee Retention',fontsize=16);\n",
    "\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "              ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(16)\n",
    "\n",
    "    ax.yaxis.set_label_coords(-0.1,0.5)\n",
    "    ax.xaxis.set_label_coords(0.5,-0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Balance of Classes\n",
    "- Look at proportion of classes to see if we're dealing with balanced or imbalanced data, since each one has its own set of tools to be used when fitting classifiers\n",
    "\n",
    "# Get number of Yes/No Attritions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit RF classifier\n",
    "clf_rf = RandomForestClassifier(n_estimators=100,\n",
    "                                criterion=\"gini\",\n",
    "                                max_features='log2',\n",
    "                                min_samples_leaf=1,\n",
    "                                class_weight=\"balanced\",\n",
    "                                n_jobs=-1,\n",
    "                                random_state=123)\n",
    "\n",
    "\n",
    "clf_rf.fit(StandardScaler().fit_transform(X_smoted), y_smoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dataframe\n",
    "with open('./data/ibm_df.pickle', 'rb') as file:\n",
    "        ibm_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot features importance\n",
    "importances = clf_rf.feature_importances_\n",
    "indices = np.argsort(clf_rf.feature_importances_)[::-1]\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(range(1,41), importances[indices], align=\"center\")\n",
    "plt.yticks(range(1,41), ibm_df.columns[ibm_df.columns != \"Attrition\"][indices], rotation=0)\n",
    "plt.xlabel('Genre', fontsize=5)\n",
    "plt.title(\"IBM Employee Attrition Feature Importance\", {\"fontsize\": 18});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = list(ibm_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_list = [ ]\n",
    "for idx, value in enumerate(df_columns):\n",
    "    df_columns_list.append([value,idx])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
